{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wPC1o67DGc1c",
        "HZf1Kpf3BzcZ",
        "_GMhuG9fG4BX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "<small>Sources des définitions : vidéos Youtube de HuggingFace.</small><br><br>\n",
        "Qu'est-ce qu'un tokenizer ?\n",
        "<br>En NLP, les données traitée sont du texte brut et cela ne permet pas aux modèles d'apprentissages de les lire ou les comprendre, ils ne peuvent travailler qu'avec des nombres. <br>\n",
        "Le tokenizer traduit le texte en chiffres.<br>\n",
        "## Types de tokenization :\n",
        "<li>word-based\n",
        "<li>character-based\n",
        "<li>subword-based (celle utilisé par GPT2Tokenizer)\n",
        "\n",
        "### Subword-based\n",
        "Le but de cette tokenization est de trouver un juste milieu entre le word-based (vocabulaire large) et le character-based (longue séquences). <br>Les mots les plus utilisés ne doivent pas être divisés en sous-mots mais seulement les mots rarement utilisés (ex: pour le mot dog non mais pour le mot dogs on le décompose en dog + s).\n",
        "<br><br>GPT2Tokenizer utilise l'algorithme Byte-Pair Encoding\n"
      ],
      "metadata": {
        "id": "wPC1o67DGc1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2Tokenizer\n",
        "```\n",
        "GPT2Tokenizer(vocabulary, merges)\n",
        "```\n",
        "\n",
        "```\n",
        "from_preset(preset)\n",
        "```\n",
        "instancie le tokenizer GPT2Tokenizer avec un vocabulaire prédéfini.\n",
        "\n",
        "`preset` : \"gpt2_base_en\", \"gpt2_medium_en\", \"gpt2_large_en\", \"gpt2_extra_large_en\", \"gpt2_base_en_cnn_dailymail\"\n",
        "\n",
        "<br>Plus de détails sur chaque preset <a href=\"https://keras.io/api/keras_nlp/models/gpt2/gpt2_tokenizer/#:~:text=Preset%20name,Description\">ici</a>"
      ],
      "metadata": {
        "id": "HZf1Kpf3BzcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp"
      ],
      "metadata": {
        "id": "8OEEl-rJdDsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c651dc3-a412-42b0-c977-c341d28ae76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.6.3-py3-none-any.whl (584 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/584.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/584.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.5/584.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-core (from keras_nlp)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (23.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2023.6.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.8)\n",
            "Collecting tensorflow-text (from keras_nlp)\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex (from keras-core->keras_nlp)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (0.15.0)\n",
            "Collecting tensorflow<2.16,>=2.15.0 (from tensorflow-text->keras_nlp)\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.59.2)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.2.2)\n",
            "Installing collected packages: namex, tensorflow-estimator, keras, keras-core, tensorboard, tensorflow, tensorflow-text, keras_nlp\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "Successfully installed keras-2.15.0 keras-core-0.1.7 keras_nlp-0.6.3 namex-0.0.7 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-text-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `vocabulary`\n",
        "  associe chaque mot (token) du vocabulaire en entiers (integer ids)\n",
        "  - type :\n",
        "    - chaîne : chemin vers un fichier JSON\n",
        "    - dictionnaire\n",
        "\n",
        "Dans le vocabulaire, il faut fournir inclure `'<|endoftext|>'` sinon l'on obtient l'erreur :\n",
        "<blockquote>Please provide `'<|endoftext|>'` in your `vocabulary` or use a pretrained `vocabulary` name.</blockquote>"
      ],
      "metadata": {
        "id": "Y4uX2MusDlUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_nlp.models import GPT2Tokenizer\n",
        "vocab = {\"chat\": 0, \"chien\": 1, \"oiseau\": 2, '<|endoftext|>': 3}\n",
        "# tokenizer = keras_nlp.models.GPT2Tokenizer(vocabulary=vocab, merges=merges)"
      ],
      "metadata": {
        "id": "RXhy5WlpDlUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf40750-111a-4562-9013-01360381ebe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `merges`\n",
        "  contient les règles de fusion\n",
        "  - type :\n",
        "    - chaîne : lien vers un fichier qui contient les règles de fusion\n",
        "      - une règle de fusion par ligne\n",
        "        - une règle de fusion a des entités séparés par des espaces\n",
        "    - liste (voir exemple ci-dessous)"
      ],
      "metadata": {
        "id": "1U3y33AEDSdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# la règle \"chat chien\" pourrait signifier que les occurrences de \"chat\" et \"chien\" dans le texte doivent être fusionnées en un seul token.\n",
        "merges = [\"chat chien\", \"oiseau vol\", \"soleil éclat\"] # ex : Le chat_chien aboit ou miaule ?\n",
        "tokenizer = GPT2Tokenizer(vocabulary=vocab, merges=merges)"
      ],
      "metadata": {
        "id": "To9NY0m7DXLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Bonjour chat chien\") # tf.Tensor en sortie si on fournit une chaîne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMEjxTRBl3C4",
        "outputId": "bfe9ea2e-ca20-4a3d-bed4-6a76f9f2f033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(18,), dtype=int32, numpy=\n",
              "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer du texte\n",
        "tokenizer(\"Thyroid surgery in children in a single institution from Osama Ibrahim Almosallama Ali Aseerib Ahme\")"
      ],
      "metadata": {
        "id": "aLihtqyHYpSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eee9ffc-1364-421e-dbce-14cd1caa57f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(99,), dtype=int32, numpy=\n",
              "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Détokenizer\n",
        "tokenizer.detokenize([  817,    88,  3882,  8185,   287,  1751,   287,   257,  2060,\n",
        "        9901,   422, 29236, 30917,   978, 16785,   439,  1689, 12104,\n",
        "         317,   325,   263,   571,  7900,  1326])"
      ],
      "metadata": {
        "id": "aG0W21_OZwkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0859ccfb-1202-47ee-c7e0-fad127b2ec31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b''>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2Preprocessor\n",
        "\n",
        "Tokenize et regroupe les inputs <br><br>\n",
        "On peut utiliser cette couche avec `tf.data.Dataset.map` pour préprocesser une chaîne au format `(x,y,sample_weight)` notamment utilisé par `Model.fit` de Keras. <br>\n",
        "`GPT2Preprocessor` ne prend qu'un segment (une chaîne) en entrée car il est principalement utilisé pour la génération de tâches  (rechercher une information, rédiger une réponse, demander une explication, ...)\n",
        "```python\n",
        "preprocessor = GPT2Preprocessor(tokenizer, sequence_length=1024, add_start_token=True, add_end_token=True)\n",
        "\n",
        "```\n",
        "- `tokenizer` : une instance de la classe `GPT2Tokenizer`\n",
        "- `sequence_length` : la taille des inputs regroupés\n",
        "- `add_start_token` : Lorsque c'est `True`, le préprocesseur ajoutera le mot de début du tokenizer à chaque séquence d'entrée. Ce mot peut marquer le début d'une phrase.\n",
        "- `add_end_token` : Lorsque c'est `True`, le mot de fin du tokenizer à chaque séquence d'entrée. Ce mot peut marquer la fin d'une phrase.\n",
        "\n",
        "## Propriétés\n",
        "`tokenizer` : GPT2.tokenizer, le tokenizer utilisé\n",
        "<br><br>\n",
        "```\n",
        "preprocessor(x,y,sample_weight)\n",
        "```\n",
        "- `x` : chaîne, `tf.Tensor`, liste de chaînes/tenseurs\n",
        "- `y` (optionnel) : tout type de donnée de label (transmis sans être modifié par le préprocesseur)\n",
        "- `sample_weight` (optionnel) : tout type de donnée de poids (transmis sans être modifié par le préprocesseur)\n",
        "- `sequence_length`: remplacer celle configurée par la couche\n",
        "```\n",
        "from_preset(preset)\n",
        "```\n"
      ],
      "metadata": {
        "id": "OL9GJwE8u1Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_nlp.models import GPT2Preprocessor\n",
        "preprocessor = GPT2Preprocessor(tokenizer=tokenizer)\n",
        "preprocessed_input = preprocessor([\"The quick brown fox jumped.\", \"Then, he crossed the road.\"])"
      ],
      "metadata": {
        "id": "huV6Iso18HMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor.tokenizer # tokenizer property"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub60Jksr8-8u",
        "outputId": "d3618cf8-ea37-4b3c-92b5-c69b370faa36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_nlp.src.models.gpt2.gpt2_tokenizer.GPT2Tokenizer at 0x7db1b1c5bcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2Backbone model\n",
        "\n",
        "Réseau de neurones GPT-2 de base avec des hyperpamètres (paramètres qui ne sont pas appris à partir des données mais réglés avant l'entrainement). <br><br>\n",
        "Le constructeur par défaut donne un modèle GPT-2 customisable et ses paramètres initialisés aléatoirement. On peut utiliser la méthode `from_preset`\n",
        "<br><br>\n",
        "Le modèle est fourni par un tiers et sujet à une [licence](https://github.com/openai/gpt-2)\n",
        "```python\n",
        "model = GPT2Backbone(\n",
        "    vocabulary_size,\n",
        "    num_layers,\n",
        "    num_heads,\n",
        "    hidden_dim,\n",
        "    intermediate_dim,\n",
        "    dropout=0.1,\n",
        "    max_sequence_length=1024\n",
        ")\n",
        "```\n",
        "- `vocabulary_size` (int) : taille du vocabulaire\n",
        "- `num_layers` (int) : nombre de couche du transformer\n",
        "- `num_heads` (int) : int. Le nombre de attention heads (?) pour chaque transformer. Ce doit être un diviseur de `hidden_size`\n",
        "- `hidden_dim` (int) : La taille de l'encodage du transformer et ses couches de **pooling**.\n",
        "- `intermediate_dim` (int): la dimension en sortie de la première couche Dense dans un réseau feedforward à deux couches pour chaque transformateur.\n",
        "- `dropout` (float): la probabilité d'abandon pour l'encodeur du transformeur\n",
        "- max_sequence_length (int) : None (utilise la valeur de `sequence_length` (GPT2Preprocessing)), la taille maximum de la séquence que l'encodeur peut accepter\n",
        "\n",
        "## Propriétés\n",
        "`token_embedding` : GPT2Backbone.token_embedding, instance de `keras.layers.Embedding`\n",
        "<br><br>\n",
        "\n",
        "```python\n",
        "from_preset(preset, load_weights=True)\n",
        "```\n",
        "- `preset` : gpt2_base_en, ...\n",
        "- `load_weights`: Par défaut à True, charger les poids pré-entrainés dans le modèle"
      ],
      "metadata": {
        "id": "Q3X6OAR8Gfda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_nlp.models import GPT2Backbone\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7u-48n7VCOtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = {\n",
        "    \"token_ids\": np.ones(shape=(1, 12), dtype=\"int32\"),\n",
        "    \"padding_mask\": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),\n",
        "}\n",
        "\n",
        "# Initialisation aléatoire de GPT-2 avec une configuration personnalisée.\n",
        "model = GPT2Backbone(\n",
        "    vocabulary_size=50257,\n",
        "    num_layers=12,\n",
        "    num_heads=12,\n",
        "    hidden_dim=768,\n",
        "    intermediate_dim=3072,\n",
        "    max_sequence_length=1024,\n",
        ")\n",
        "model(input_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No4lKGd3GKJl",
        "outputId": "8cca9736-07a0-4645-e4cf-c060b2e88e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 12, 768), dtype=float32, numpy=\n",
              "array([[[ 0.47852927, -0.65031123,  1.496662  , ...,  1.058555  ,\n",
              "         -0.9930843 ,  2.0575511 ],\n",
              "        [ 0.97722244, -0.36336008,  1.7122997 , ...,  1.3312082 ,\n",
              "         -1.0386397 ,  1.5186324 ],\n",
              "        [ 1.3595175 , -0.13302547,  1.9602071 , ...,  1.2170106 ,\n",
              "         -0.6331256 ,  1.5580916 ],\n",
              "        ...,\n",
              "        [ 0.7286012 , -0.11738614,  1.1361239 , ...,  0.00574119,\n",
              "         -0.44189125,  1.788145  ],\n",
              "        [ 0.79080987, -0.25844705,  1.4011316 , ...,  0.08362782,\n",
              "         -0.24227712,  1.6321561 ],\n",
              "        [ 0.6290515 , -0.33386242,  1.3729063 , ...,  0.04617018,\n",
              "         -0.22661753,  1.8361477 ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ht8h3G-Un8ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Backbone.from_preset(\"gpt2_base_en\")\n",
        "\n",
        "output = model(preprocessed_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "57y_ezpJFTkl",
        "outputId": "991d5d03-a356-44a1-967a-fc14a3a5dc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-1c423ec0f290>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Backbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_preset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2_base_en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_nlp/src/layers/modeling/reversible_embedding.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, reverse)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'token_embedding' (type ReversibleEmbedding).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1,1] = -1 is not in [0, 50257) [Op:ResourceGather] name: \n\nCall arguments received by layer 'token_embedding' (type ReversibleEmbedding):\n  • inputs=tf.Tensor(shape=(2, 1024), dtype=int32)\n  • reverse=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h4AXlDNg6VPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.token_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLvNyPzy-4xN",
        "outputId": "2aba07b1-cfa5-4024-ae97-d0a85a6d84d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_nlp.src.layers.modeling.reversible_embedding.ReversibleEmbedding at 0x7db1b13fbfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2CausalLMPreprocessor layer\n",
        "**GPT2CausalLMPreprocessor** class\n",
        "```\n",
        "keras_nlp.models.GPT2CausalLMPreprocessor(\n",
        "    tokenizer, sequence_length=1024, add_start_token=True, add_end_token=True\n",
        ")\n",
        "```\n",
        "<br>La couche de pré-processing qui est utilisée par `GPT2CausalLM`\n",
        "\n",
        "\n",
        "Arguments\n",
        "- `tokenizer` = une instance de GPT2Tokenizer\n",
        "- `sequence_length` = la taille des \"packed input\"\n",
        "- `add_start_token`\n",
        "- `add_end_token`\n",
        "\n",
        "<br>Cette couche de pré-processing est pour être utilisée par GPT2CausalLM que nous verrons par la suite.\n",
        "Returne au format (x, y, sample_weight)\n",
        "- `x` : string, un tenseur ou un tableau de string\n",
        "- `y` : label, toujours à `None` pour la couche\n",
        "- `sample_weight` : string, un tenseur ou un tableau de string\n",
        "- `sequence_length` : remplace `sequence_length` prédéfini\n",
        "<br>\n",
        "\n",
        "Ces arguments sont utilisées lors de l'appel du preprocessor\n",
        "```\n",
        "lmP = keras_nlp.models.GPT2CausalLMPreprocessor(\n",
        "    tokenizer, sequence_length=1024, add_start_token=True, add_end_token=True\n",
        ")\n",
        "lmP(x, y, sample_weight, sequence_length)\n",
        "```\n",
        "<br><br>\n",
        "\n",
        "\n",
        "Méthodes\n",
        "- `from_preset()`\n",
        "- `tokenizer()`\n",
        "- `generate_preprocess(x, sequence_length=None)`\n",
        "- `generate_postprocess(x)`\n",
        "<br><br>\n",
        "Attaché à GPT2CausalLM, ces méthodes (`generate_preprocess` et `generate_postprocess`) sont appelées implicitement par `generate()` de `GPT2CausalLM`"
      ],
      "metadata": {
        "id": "_GMhuG9fG4BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_nlp.models import GPT2CausalLMPreprocessor\n",
        "import tensorflow as tf\n",
        "# Load the preprocessor from a preset.\n",
        "preprocessor = GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\"\n",
        ")\n",
        "\n",
        "# Tokenize and pack a single sentence.\n",
        "sentence = tf.constant(\"League of legends\")\n",
        "preprocessor(sentence)\n",
        "# Same output.\n",
        "preprocessor(\"League of legends\")\n",
        "\n",
        "# Tokenize a batch of sentences.\n",
        "sentences = tf.constant([\"Taco tuesday\", \"Fish taco please!\"])\n",
        "preprocessor(sentences)\n",
        "# Same output.\n",
        "preprocessor([\"Taco tuesday\", \"Fish taco please!\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZLDIsiLQfz6",
        "outputId": "42a073c3-52d3-48ec-b83a-a3a289a7e300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'token_ids': <tf.Tensor: shape=(2, 1024), dtype=int32, numpy=\n",
              "  array([[50256,    51, 10602, ...,     0,     0,     0],\n",
              "         [50256, 39428, 47884, ...,     0,     0,     0]], dtype=int32)>,\n",
              "  'padding_mask': <tf.Tensor: shape=(2, 1024), dtype=bool, numpy=\n",
              "  array([[ True,  True,  True, ..., False, False, False],\n",
              "         [ True,  True,  True, ..., False, False, False]])>},\n",
              " <tf.Tensor: shape=(2, 1024), dtype=int32, numpy=\n",
              " array([[   51, 10602,   256, ...,     0,     0,     0],\n",
              "        [39428, 47884,  3387, ...,     0,     0,     0]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(2, 1024), dtype=bool, numpy=\n",
              " array([[ True,  True,  True, ..., False, False, False],\n",
              "        [ True,  True,  True, ..., False, False, False]])>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Map a dataset to preprocess a single sentence.\n",
        "features = tf.constant(\n",
        "    [\n",
        "        \"Avatar 2 is amazing!\",\n",
        "        \"Well, I am not sure.\",\n",
        "    ]\n",
        ")\n",
        "labels = tf.constant([1, 0])\n",
        "ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Map a dataset to preprocess unlabled sentences.\n",
        "ds = tf.data.Dataset.from_tensor_slices(features)\n",
        "ds = ds.map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGWtH75t6dub",
        "outputId": "3aa13d42-c10e-4ef0-f716-99624b57bee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`GPT2CausalLMPreprocessor` generates `y` and `sample_weight` based on your input data, but your data already contains `y` or `sample_weight`. Your `y` and `sample_weight` will be ignored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2CausalLM layer\n",
        "**GPT2CausalLM** class\n",
        "```\n",
        "keras_nlp.models.GPT2CausalLM (backbone, preprocessor=None)\n",
        "```\n",
        "- `backbone` : instance de `GPT2Backbone`\n",
        "- `preprocessor` : instance de `GPT2Preprocessor`. Si c'est `None`, alors il faudra faire du préprocesser à chaque fois les inputs avant d'appeler le modèle\n",
        "<br><br>\n",
        "\n",
        "Modèle de langage causal prédit le prochain token à partir des token précédents.\n",
        "<br>Pourquoi l'utiliser ? entraîner un modèle non supervisé (pas de labels, pas de y) sur du text brut ou générer de manière autorégressive du text brut\n",
        "<br> Dans quel usage ? pré-entrainer ou fine-tuner un modèle GPT-2 avec une méthode `fit()`\n",
        "\n",
        "\n",
        "Le modèle peut être configuré de manière optionnelle avec une couche `preprocessor`(GPT2CausalLMPreprocessor). Cela permet d'automatiquement appliquer le preprocessing sur les texte en entrée dans les méthodes `fit()`, `predict()`, `evaluate()` and `generate()`\n",
        "<br><br>\n",
        "\n",
        "- `from_preset(presetn load_weights)`\n",
        "- `generate(inputs, max_length=None)` : génère du texte à partir d'un prompt\n",
        "  - `inputs` : donnée python ou de tenseur ou `tf.data.Dataset`\n",
        "  - `max_length` (optionnel) : taille maximal en entier de la séquence générée. Par défaut, c'est la valeur `sequence_length` de `GPT2CausalLMPreprocessor`  \n",
        "- `compile()` : contrôler la génération de texte avec un argument [`sampler`](https://keras.io/api/keras_nlp/samplers/) (la méthode de sampling utilisée pour la génération) (top_k est utilisé par défaut)\n",
        "- `fit()` : entraîner le modèle\n",
        "- `predict()` : faire des prédictions pour le modèle\n",
        "- `evaluate()` : évaluer le modèle\n",
        "\n",
        "## Arguments\n",
        "- `backbone` : une instance de `keras.Model` qui fournit le sous-model backbone\n",
        "- `preprocessor` : une instance de `keras.layers.Layer` utilisée pour préprocesser les inputs"
      ],
      "metadata": {
        "id": "2V_J0d98hAr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_nlp.models import GPT2CausalLM\n",
        "\n",
        "gpt2_lm = GPT2CausalLM.from_preset(\"gpt2_base_en\")\n",
        "gpt2_lm.generate(\"I want to say\", max_length=30)\n",
        "\n",
        "# Generate with batched prompts.\n",
        "gpt2_lm.generate([\"This is a\", \"Where are you\"], max_length=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZpHwd4Fpsqd",
        "outputId": "30c64fce-0af7-4770-9d75-80064df91353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/vocab.json\n",
            "1042301/1042301 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/merges.txt\n",
            "456318/456318 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/model.h5\n",
            "497986112/497986112 [==============================] - 8s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is a guest post by Michael D. Gellman, PhD, professor of psychology at the University of Pennsylvania and coauthor of The Brain',\n",
              " \"Where are you?\\n\\nWe are in the midst of a major renovation, and the building's new interior and exterior have been completely remodeled\"]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp.samplers\n",
        "\n",
        "gpt2_lm = GPT2CausalLM.from_preset(\"gpt2_base_en\")\n",
        "gpt2_lm.compile(sampler=\"greedy\")\n",
        "gpt2_lm.generate(\"I want to say\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "RX9gFw5QtzZq",
        "outputId": "f8a22248-9a0e-4ca3-845d-7403eca2dafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <bound method GPT2CausalLM.generate_step of <keras_nlp.src.models.gpt2.gpt2_causal_lm.GPT2CausalLM object at 0x7f80b95ca290>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I want to say that I am not a fan of the idea of a \"real\" world. I am a fan of the idea of a \"real\" world. I am a fan of the idea of a \"real\" world. I'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_lm.compile(sampler=keras_nlp.samplers.BeamSampler(num_beams=2))\n",
        "gpt2_lm.generate(\"I want to say\", max_length=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FDDLyUWhuq9F",
        "outputId": "4855e879-c174-49cb-83a2-37f0aa55a2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I want to say a few things about this book. First, it is a very good book. It is a very good book. It is'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Prompt the model with `5338, 318` (the token ids for `\"Who is\"`).\n",
        "# Use `\"padding_mask\"` to indicate values that should not be overridden.\n",
        "prompt = {\n",
        "    \"token_ids\": np.array([[5338, 318, 0, 0, 0]] * 2),\n",
        "    \"padding_mask\": np.array([[1, 1, 0, 0, 0]] * 2),\n",
        "}\n",
        "\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    preprocessor=None,\n",
        ")\n",
        "gpt2_lm.generate(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtLI4YfZw4Iy",
        "outputId": "0b1f7f86-dd3b-461d-dd36-aff41ef146a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'token_ids': array([[5338,  318,  428, 3516, 1701],\n",
              "        [5338,  318,  262,  582,   30]]),\n",
              " 'padding_mask': array([[ True,  True,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True]])}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\"The quick brown fox jumped.\", \"I forgot my homework.\"]\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\")\n",
        "gpt2_lm.fit(x=features, batch_size=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyePFfVfxaOS",
        "outputId": "26fae701-2e48-4916-ff1f-8eddb9bad50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 166s 166s/step - loss: 0.0367 - sparse_categorical_accuracy: 9.7656e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f80b91c8070>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_lm.generate(\"I want to say\", max_length=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "RVrVtgLMySXu",
        "outputId": "5a1b5614-8327-4c5f-d59e-c79b56ab1584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I want to say a big THANK YOU to all the people who supported me on my first project and to my wife who helped me get my ideas into the hands of people who were willing to work with me. I'm so glad to see that I got a lot of support from all of you and to all of those I've met. I hope you're all happy with the results you've achieved. Thank you for all of your support and thank you for your patience.\\n\\n\\nI hope\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2 en réponse à des stimuli\n",
        "\n"
      ],
      "metadata": {
        "id": "MFCn3YlfwOCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp"
      ],
      "metadata": {
        "id": "6EOROBJHyugl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras_nlp.models import GPT2CausalLM, GPT2CausalLMPreprocessor\n",
        "\n",
        "gpt2_lm = GPT2CausalLM.from_preset(\"gpt2_base_en\")\n",
        "def QA(input,max_length=60):\n",
        "  return gpt2_lm.generate(input, max_length)[len(input):]\n",
        "print(QA('You are a cancer dectector robot and i want to know if i have a thyroid cancer?',100))"
      ],
      "metadata": {
        "id": "FEnob5x-BcEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c760b47-e743-40cf-b2d0-65de74c19757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n",
            "\n",
            "\n",
            "You can help by sharing your story with the world.\n",
            "\n",
            "I have thyroid cancer, I have an infection, and i have been treated for it. What is it?\n",
            "\n",
            "I have been treated for thyroid cancer since January 2011. I am now on a waiting list. How does it work?\n",
            "\n",
            "You have to register for your cancer diagnosis. If you don't,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(QA('what is a thyroid cancer ?',100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WAK5FveDz8K",
        "outputId": "a40151ac-7097-453e-dfe3-618c6f661c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "a thyroid cancer is a disease caused by a thyroid defect that causes abnormal growth of thyroid tissue.\n",
            "\n",
            "a thyroid defect that causes abnormal growth of thyroid tissue. a thyroid disease is an inherited condition, which means you can't develop it if you don't have a thyroid defect or if you develop a disease that affects your thyroid tissue or the thyroid gland itself.\n",
            "\n",
            "a thyroid defect or an inherited condition, which means you can't develop it if you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(QA('how can i know that i have a lung cancer ?',100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3cYiRXjEck_",
        "outputId": "80ed1f51-8f29-4d1c-b916-e102e0333303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "how can i know if my cancer has spread ?\n",
            "\n",
            "how can i know when my cancer has spread ?\n",
            "\n",
            "what are the steps i need to take to get my cancer diagnosed ?\n",
            "\n",
            "how can i help my cancer get treated ?\n",
            "\n",
            "what can i do to help my cancer ?\n",
            "\n",
            "what should i do if my cancer is spreading ?\n",
            "\n",
            "why do i have cancer ?\n",
            "\n",
            "how do i get\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(QA('How many types of cancer ?',200))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfsD2RDUGTLx",
        "outputId": "1b157151-b6d7-4132-cef0-a4775b639912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "A few people say that there are only four types of cancer, and that there's no reason to think there are only four types of cancer.\n",
            "\n",
            "The most common cancers are:\n",
            "\n",
            "Tumor (cancerous tissue in the abdomen or neck)\n",
            "\n",
            "Tumoral (small, small, or large)\n",
            "\n",
            "Tumoral (small)\n",
            "\n",
            "Vascular (small, large)\n",
            "\n",
            "Cancer of the mouth\n",
            "\n",
            "Tumor of the throat\n",
            "\n",
            "Tumor of the liver\n",
            "\n",
            "Vascular cancer\n",
            "\n",
            "The most common cancers are:\n",
            "\n",
            "Tumor of the throat\n",
            "\n",
            "Tumoral of the lungs\n",
            "\n",
            "Vascular cancer\n",
            "\n",
            "The most common cancers are:\n",
            "\n",
            "Tumor of the liver\n",
            "\n",
            "Vascular cancer\n",
            "\n",
            "Vascular cancer\n",
            "\n",
            "The most common cancers are:\n",
            "\n",
            "Tumor of the lung\n",
            "\n",
            "Vascular cancer\n",
            "\n",
            "The most common\n"
          ]
        }
      ]
    }
  ]
}