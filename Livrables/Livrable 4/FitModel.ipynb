{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/](https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/)"
      ],
      "metadata": {
        "id": "IoOghrrmunR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HhbD-p9my-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f33139-21e7-4cef-850f-980c7fe5bc7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: namex, keras-core, tensorflow-text, keras_nlp\n",
            "Successfully installed keras-core-0.1.7 keras_nlp-0.7.0 namex-0.0.7 tensorflow-text-2.15.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "bigframes 0.19.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.1.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install keras_nlp pandas keras\n",
        "!pip install -q --upgrade keras_nlp pandas keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jR0JVBa-rubq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525d7aed-586c-4a57-d1c4-ebaafc3b48a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras_nlp.metrics import Perplexity\n",
        "from keras_nlp.models import GPT2CausalLMPreprocessor, GPT2CausalLM\n",
        "from keras import mixed_precision"
      ],
      "metadata": {
        "id": "j-sDZZpmnxom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e8a110-fa67-4348-82fc-67409c67ce2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPUMemoryCallback(keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        target_batches,\n",
        "        print_stats=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.target_batches = target_batches\n",
        "        self.print_stats = print_stats\n",
        "\n",
        "        self.memory_usage = []\n",
        "        self.labels = []\n",
        "\n",
        "    def _compute_memory_usage(self):\n",
        "        memory_stats = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
        "        # Convert bytes to GB and store in list.\n",
        "        peak_usage = round(memory_stats[\"peak\"] / (2**30), 3)\n",
        "        self.memory_usage.append(peak_usage)\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self._compute_memory_usage()\n",
        "        self.labels.append(f\"epoch {epoch} start\")\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        if batch in self.target_batches:\n",
        "            self._compute_memory_usage()\n",
        "            self.labels.append(f\"batch {batch}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self._compute_memory_usage()\n",
        "        self.labels.append(f\"epoch {epoch} end\")"
      ],
      "metadata": {
        "id": "Zk0_V_ANrnIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoraLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        rank=8,\n",
        "        alpha=32,\n",
        "        trainable=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # We want to keep the name of this layer the same as the original\n",
        "        # dense layer.\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self._num_heads = original_layer_config[\"output_shape\"][-2]\n",
        "        self._hidden_dim = self._num_heads * original_layer_config[\"output_shape\"][-1]\n",
        "\n",
        "        # Layers.\n",
        "\n",
        "        # Original dense layer.\n",
        "        self.original_layer = original_layer\n",
        "        # No matter whether we are training the model or are in inference mode,\n",
        "        # this layer should be frozen.\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "        # LoRA dense layers.\n",
        "        self.A = keras.layers.Dense(\n",
        "            units=rank,\n",
        "            use_bias=False,\n",
        "            # Note: the original paper mentions that normal distribution was\n",
        "            # used for initialization. However, the official LoRA implementation\n",
        "            # uses \"Kaiming/He Initialization\".\n",
        "            kernel_initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_A\",\n",
        "        )\n",
        "        # B has the same `equation` and `output_shape` as the original layer.\n",
        "        # `equation = abc,cde->abde`, where `a`: batch size, `b`: sequence\n",
        "        # length, `c`: `hidden_dim`, `d`: `num_heads`,\n",
        "        # `e`: `hidden_dim//num_heads`. The only difference is that in layer `B`,\n",
        "        # `c` represents `rank`.\n",
        "        self.B = keras.layers.EinsumDense(\n",
        "            equation=original_layer_config[\"equation\"],\n",
        "            output_shape=original_layer_config[\"output_shape\"],\n",
        "            kernel_initializer=\"zeros\",\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_B\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_output = self.original_layer(inputs)\n",
        "        if self.trainable:\n",
        "            # If we are fine-tuning the model, we will add LoRA layers' output\n",
        "            # to the original layer's output.\n",
        "            lora_output = self.B(self.A(inputs)) * self._scale\n",
        "            return original_output + lora_output\n",
        "\n",
        "        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n",
        "        # the original layer's weights - more on this in the text generation\n",
        "        # section!\n",
        "        return original_output"
      ],
      "metadata": {
        "id": "W9yPVRGbrmQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch on mixed precision\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "6oFTxqetnyeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch on XLA\n",
        "tf.config.optimizer.set_jit(True)"
      ],
      "metadata": {
        "id": "K561WrqNpQ_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "NUM_BATCHES = 500\n",
        "EPOCHS = 10  # Can be set to a higher value for better results\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "MAX_GENERATION_LENGTH = 200\n",
        "\n",
        "GPT2_PRESET = \"gpt2_base_en\"\n",
        "\n",
        "# LoRA-specific hyperparameters\n",
        "RANK = 4\n",
        "ALPHA = 32.0"
      ],
      "metadata": {
        "id": "UtHccejHsDgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "csv_path = '/content/drive/MyDrive/Data/CleanedMedicalDataset.csv'\n",
        "\n",
        "columns = [\"Serial Number\",\"Class Labels\",\"Research Paper Text\"]\n",
        "dataFrame = pd.read_csv(csv_path, header=None, encoding = \"latin-1\", names=columns)"
      ],
      "metadata": {
        "id": "E-S4CIfRr2EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "\n",
        "# This resets \"peak\" memory usage to \"current\" memory usage.\n",
        "tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
        "\n",
        "preprocessor = GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "lora_model = GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    preprocessor=preprocessor,\n",
        ")"
      ],
      "metadata": {
        "id": "f2LBfx3astGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac02759b-9688-4c27-d8dd-5b3935e860d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/tokenizer.json...\n",
            "100%|██████████| 448/448 [00:00<00:00, 1.18MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/assets/tokenizer/merges.txt...\n",
            "100%|██████████| 446k/446k [00:00<00:00, 539kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/assets/tokenizer/vocabulary.json...\n",
            "100%|██████████| 0.99M/0.99M [00:01<00:00, 1.04MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/config.json...\n",
            "100%|██████████| 484/484 [00:00<00:00, 1.23MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.weights.h5...\n",
            "100%|██████████| 475M/475M [00:32<00:00, 15.5MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# override the original query/value projection matrices with our new LoRA layers\n",
        "for layer_idx in range(lora_model.backbone.num_layers):\n",
        "    # Change query dense layer.\n",
        "    decoder_layer = lora_model.backbone.get_layer(f\"transformer_layer_{layer_idx}\")\n",
        "    self_attention_layer = decoder_layer._self_attention_layer\n",
        "    # Allow mutation to Keras layer state.\n",
        "    # self_attention_layer._tracker.locked = False\n",
        "\n",
        "    # Change query dense layer.\n",
        "    self_attention_layer._query_dense = LoraLayer(\n",
        "        self_attention_layer._query_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "    # Change value dense layer.\n",
        "    self_attention_layer._value_dense = LoraLayer(\n",
        "        self_attention_layer._value_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "6LXjLnafuf9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do a forward pass to make sure we still have a valid chain of computation.\n",
        "lora_model(preprocessor([\"LoRA is very useful for quick LLM finetuning\"])[0])\n",
        "pass"
      ],
      "metadata": {
        "id": "SIlPepDQuwCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the entire LLM, only the LoRA layers should be trainable\n",
        "for layer in lora_model._flatten_layers():\n",
        "    lst_of_sublayers = list(layer._flatten_layers())\n",
        "\n",
        "    if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
        "        if layer.name in [\"lora_A\", \"lora_B\"]:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False"
      ],
      "metadata": {
        "id": "64mmbNlcuyMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.summary()"
      ],
      "metadata": {
        "id": "zkoTQoEYvMiq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "383d8113-e8f1-4b53-a88e-5cd7ae1ab808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                     │                                              \u001b[38;5;34m50,257\u001b[0m │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                     │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                      │     \u001b[38;5;34m124,587,264\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)                    │      \u001b[38;5;34m38,597,376\u001b[0m │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">124,587,264</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)                    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,587,264\u001b[0m (475.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,587,264</span> (475.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m147,456\u001b[0m (576.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span> (576.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and loss\n",
        "def get_optimizer_and_loss():\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        epsilon=1e-6,\n",
        "        global_clipnorm=1.0,  # Gradient clipping.\n",
        "    )\n",
        "    # Exclude layernorm and bias terms from weight decay.\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
        "\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return optimizer, loss"
      ],
      "metadata": {
        "id": "Xz0AM6OHslW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure model (essayer adam puis sgd si besoin)\n",
        "gpu_memory_callback = GPUMemoryCallback(\n",
        "    target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500],\n",
        "    print_stats=True,\n",
        ")\n",
        "\n",
        "optimizer, loss = get_optimizer_and_loss()\n",
        "\n",
        "lora_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    weighted_metrics=[Perplexity()],\n",
        ")"
      ],
      "metadata": {
        "id": "GthocYwIpV9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_steps = len(dataFrame) / BATCH_SIZE\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
        "lora_model.fit(\n",
        "    dataFrame,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[gpu_memory_callback, early_stopping],\n",
        ")\n",
        "lora_model_memory_usage = gpu_memory_callback.memory_usage\n",
        "print(f\"GPU memory usage : {gpu_memory_callback.memory_usage}\")"
      ],
      "metadata": {
        "id": "jmhO_6OVpeEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f2552f-2ef5-40bf-89b1-db492217bdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 6234s 176s/step - loss: 6.4456 - perplexity: 49690.5469\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 25s 722ms/step - loss: 6.2783 - perplexity: 49528.2695\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 26s 755ms/step - loss: 6.1400 - perplexity: 49250.6094\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 32s 964ms/step - loss: 6.0450 - perplexity: 49103.4375\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 26s 775ms/step - loss: 5.9657 - perplexity: 49255.3047\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 25s 740ms/step - loss: 5.9017 - perplexity: 49147.2461\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 25s 728ms/step - loss: 5.8479 - perplexity: 49198.8281\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 25s 736ms/step - loss: 5.8015 - perplexity: 49245.4414\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 26s 760ms/step - loss: 5.7522 - perplexity: 49373.4414\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 26s 793ms/step - loss: 5.7141 - perplexity: 49495.2188\n",
            "GPU memory usage : [0.787, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538, 4.538]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge weights\n",
        "for layer_idx in range(lora_model.backbone.num_layers):\n",
        "    self_attention_layer = lora_model.backbone.get_layer(\n",
        "        f\"transformer_layer_{layer_idx}\"\n",
        "    )._self_attention_layer\n",
        "\n",
        "    # Merge query dense layer.\n",
        "    query_lora_layer = self_attention_layer._query_dense\n",
        "\n",
        "    A_weights = query_lora_layer.A.kernel  # (768, 1) (a, b)\n",
        "    B_weights = query_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
        "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
        "    query_lora_layer.original_layer.kernel.assign_add(increment_weights)\n",
        "\n",
        "    # Merge value dense layer.\n",
        "    value_lora_layer = self_attention_layer._value_dense\n",
        "\n",
        "    A_weights = value_lora_layer.A.kernel  # (768, 1) (a, b)\n",
        "    B_weights = value_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
        "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
        "    value_lora_layer.original_layer.kernel.assign_add(increment_weights)"
      ],
      "metadata": {
        "id": "oM3vluqdvp1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "lora_model.save_weights('/content/drive/MyDrive/Checkpoints/weights')"
      ],
      "metadata": {
        "id": "1EBW2O7VrHgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c5afb3-63cd-489a-ac7c-8cb932cab868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:47: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:47: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        }
      ]
    }
  ]
}